import logging
import os
from io import BytesIO

import torch
from dotenv import load_dotenv
from PIL import Image
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from transformers import AutoModelForCausalLM, AutoProcessor

# Setup logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
HF_MODEL_NAME = os.getenv("HF_MODEL_NAME", "microsoft/Florence-2-base")  # Default to Florence-2 base model

# --- AI Model Loading ---
logger.info(f"Loading model: {HF_MODEL_NAME}")
model = None
processor = None
try:
    # For Florence-2 models, we use AutoModelForCausalLM and AutoProcessor.
    # trust_remote_code=True is required for Florence-2.
    model = AutoModelForCausalLM.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
    processor = AutoProcessor.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    logger.info(f"Model loaded successfully on {device}.")
except Exception as e:
    logger.error(f"Failed to load model: {e}")
    # Exit if model fails to load
    exit()
# --- End AI Model Loading ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Sends a welcome message when the /start command is issued."""
    await context.bot.send_message(
        chat_id=update.effective_chat.id,
        text="Hello! I'm an image captioning bot. Send me an image, and I'll tell you what I see."
    )

def generate_caption(image: Image.Image) -> str:
    """Generates a caption for the given image using Florence-2 model."""
    logger.info("Generating caption for image.")

    # This function is now specifically for Florence-2 models.
    if "florence" not in HF_MODEL_NAME.lower():
        logger.warning("This caption generation function is designed for Florence-2 models. The current model is not a Florence-2 model.")
        return "This bot is configured for Florence-2 models. The current model is not a Florence-2 model."

    # User requested "middle caption setting", which corresponds to <DETAILED_CAPTION>
    task_prompt = "<DETAILED_CAPTION>"
    
    # The processor for Florence-2 handles both text and image.
    inputs = processor(text=task_prompt, images=image, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Generate caption
    max_new_tokens = 256  # Increased for more detailed captions
    num_beams = 4
    gen_kwargs = {"max_new_tokens": max_new_tokens, "num_beams": num_beams}

    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        **gen_kwargs
    )

    # The processor has a special post-processing step.
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    
    # The post_process_generation function cleans up the output.
    parsed_answer = processor.post_process_generation(
        generated_text, 
        task=task_prompt, 
        image_size=(image.width, image.height)
    )

    caption = parsed_answer.get(task_prompt, "Could not generate caption.")
    
    logger.info(f"Generated caption: '{caption}'")
    return caption

async def handle_image(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handles incoming photos and generates a caption."""
    chat_id = update.effective_chat.id
    logger.info(f"Received image from chat_id: {chat_id}")

    await context.bot.send_message(chat_id=chat_id, text="Processing your image...")

    # Get the largest photo sent
    photo_file = await update.message.photo[-1].get_file()
    
    # Download the photo into a BytesIO object
    file_bytes = await photo_file.download_as_bytearray()
    image_stream = BytesIO(file_bytes)
    
    try:
        image = Image.open(image_stream).convert("RGB")
        caption = generate_caption(image)
        await context.bot.send_message(chat_id=chat_id, text=f"{caption},  Polite Notice: this caption is generated by a locally hosted AI model on an ARM Mac Mini, not by an online service or a data center. It may not be accurate or reliable. John uses this method of alt text generation, because they have dyslexia and find writing alt-text mentally taxing")
    except Exception as e:
        logger.error(f"An error occurred while processing the image: {e}")
        await context.bot.send_message(chat_id=chat_id, text="Sorry, I couldn't process that image.")


def main():
    """Start the bot."""
    if not TELEGRAM_BOT_TOKEN:
        logger.error("TELEGRAM_BOT_TOKEN not found. Please create a .env file and add it.")
        return

    application = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()

    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.PHOTO, handle_image))

    logger.info("Bot is running. Press Ctrl-C to stop.")
    application.run_polling()


if __name__ == '__main__':
    main()
