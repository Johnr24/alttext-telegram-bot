import logging
import os
from io import BytesIO
import yaml

import torch
from dotenv import load_dotenv
from PIL import Image
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer, AutoModelForVision2Seq

# Setup logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
HF_TOKEN = os.getenv("HF_TOKEN")
HF_MODEL_NAME = os.getenv("HF_MODEL_NAME", "Qwen/Qwen2.5-VL-3B-Instruct")  # Default to Qwen2.5 VL
USER = os.getenv("USER", "the user")  # Default to 'john' if USER is not set
ALLOWED_USER_IDS_STR = os.getenv("ALLOWED_USER_IDS")
ALLOWED_USER_IDS = [int(uid.strip()) for uid in ALLOWED_USER_IDS_STR.split(',')] if ALLOWED_USER_IDS_STR else []
SYSTEM_PROMPT = os.getenv("SYSTEM_PROMPT")
FLORENCE2_TASK_PROMPT = os.getenv("FLORENCE2_TASK_PROMPT", "<DETAILED_CAPTION>")
USER_DATA_FILE = 'user_data.yaml'
DEFAULT_POLITE_NOTICE = f"Polite Notice: this caption is generated by a locally hosted AI model on an ARM Mac Mini, not by an online service or a data center. It may not be accurate or reliable. {USER} uses this method of alt text generation, because they have dyslexia and find writing alt-text mentally taxing"

# --- YAML data handling functions ---
def load_user_data():
    """Loads user data from the YAML file."""
    if not os.path.exists(USER_DATA_FILE):
        return {}
    try:
        with open(USER_DATA_FILE, 'r') as f:
            return yaml.safe_load(f) or {}
    except (yaml.YAMLError, IOError) as e:
        logger.error(f"Error loading user data from {USER_DATA_FILE}: {e}")
        return {}

def save_user_data(data):
    """Saves user data to the YAML file."""
    try:
        with open(USER_DATA_FILE, 'w') as f:
            yaml.dump(data, f, default_flow_style=False)
    except IOError as e:
        logger.error(f"Error saving user data to {USER_DATA_FILE}: {e}")


# --- Model Lazy Loading ---
# Global variables to hold the model and processor. They are loaded on demand.
model = None
processor = None
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")


def load_model_and_processor():
    """Loads the model and processor/tokenizer into memory if they are not already loaded."""
    global model, processor
    if model is None or processor is None:
        logger.info(f"Attempting to load model: {HF_MODEL_NAME} on device: {device}")
        try:
            if "florence" in HF_MODEL_NAME.lower():
                model = AutoModelForCausalLM.from_pretrained(HF_MODEL_NAME, trust_remote_code=True, token=HF_TOKEN)
                processor = AutoProcessor.from_pretrained(HF_MODEL_NAME, trust_remote_code=True, token=HF_TOKEN)
            elif "qwen" in HF_MODEL_NAME.lower():
                # For Qwen-VL models, we use AutoModelForVision2Seq and AutoProcessor.
                # Load in float16 for MPS to reduce memory footprint.
                if device.type == 'mps':
                    model = AutoModelForVision2Seq.from_pretrained(HF_MODEL_NAME, trust_remote_code=True, torch_dtype=torch.float16, token=HF_TOKEN)
                else:
                    model = AutoModelForVision2Seq.from_pretrained(HF_MODEL_NAME, trust_remote_code=True, token=HF_TOKEN)
                processor = AutoProcessor.from_pretrained(HF_MODEL_NAME, trust_remote_code=True, token=HF_TOKEN)
            else:
                raise ValueError(f"Unsupported model type: {HF_MODEL_NAME}")

            model.to(device)
            logger.info(f"Model {HF_MODEL_NAME} loaded successfully on {device}.")
        except Exception as e:
            logger.error(f"Failed to load model {HF_MODEL_NAME}: {e}")
            model = None
            processor = None
            raise

def unload_model_and_processor():
    """Unloads the model and processor from memory to free up resources."""
    global model, processor
    if model is not None or processor is not None:
        logger.info("Attempting to unload model from memory.")
        del model
        del processor
        model = None
        processor = None
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        logger.info("Model unloaded successfully.")


async def set_suffix(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Sets or removes a custom suffix for the user."""
    user_id = update.effective_user.id
    if ALLOWED_USER_IDS and user_id not in ALLOWED_USER_IDS:
        logger.warning(f"Unauthorized access attempt by user_id: {user_id} for /setsuffix")
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text="Sorry, you are not authorized to use this command."
        )
        return

    suffix_text = ' '.join(context.args)
    user_data = load_user_data()

    if not suffix_text:
        # Attempt to remove suffix
        if user_data.get('users', {}).get(user_id, {}).get('suffix') is not None:
            del user_data['users'][user_id]['suffix']
            if not user_data['users'][user_id]:  # Clean up empty user dict
                del user_data['users'][user_id]
            save_user_data(user_data)
            await context.bot.send_message(
                chat_id=update.effective_chat.id,
                text="Your custom suffix has been removed. The default will be used."
            )
        else:
            await context.bot.send_message(
                chat_id=update.effective_chat.id,
                text="You don't have a custom suffix to remove. To set one, use: /setsuffix <your message>"
            )
        return

    # Set suffix
    user_data.setdefault('users', {}).setdefault(user_id, {})['suffix'] = suffix_text
    save_user_data(user_data)

    await context.bot.send_message(
        chat_id=update.effective_chat.id,
        text="Your custom suffix has been set."
    )


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Sends a welcome message when the /start command is issued."""
    user_id = update.effective_user.id
    if ALLOWED_USER_IDS and user_id not in ALLOWED_USER_IDS:
        logger.warning(f"Unauthorized access attempt by user_id: {user_id}")
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text="Sorry, you are not authorized to use this bot."
        )
        return

    await context.bot.send_message(
        chat_id=update.effective_chat.id,
        text="Hello! I'm an image captioning bot. Send me an image, and I'll tell you what I see."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Sends a help message with all available commands."""
    help_text = (
        "Welcome to the Image Captioning Bot! Here's how to use me:\n\n"
        "To generate a caption, simply send an image.\n"
        "To guide the caption generation, provide a text prompt in the image's caption. The model will use your prompt to focus its description. For example, sending an image with the caption 'A black dog playing fetch' will guide the model.\n\n"
        "Available commands:\n"
        "/start - Get a welcome message.\n"
        "/help - Show this help message.\n"
        "/setsuffix <your message> - Set a custom suffix for your captions. "
        "To remove it, use the command without a message."
    )
    await context.bot.send_message(
        chat_id=update.effective_chat.id,
        text=help_text
    )

def generate_caption(image: Image.Image, user_prompt: str) -> str:
    """Generates a caption for the given image using the selected model."""
    logger.info(f"Generating caption for image with user prompt: '{user_prompt}'")

    try:
        load_model_and_processor()

        if "florence" in HF_MODEL_NAME.lower():
            # Florence-2 specific logic
            base_task_prompt = FLORENCE2_TASK_PROMPT
            model_prompt = f'{base_task_prompt} {user_prompt}' if user_prompt else base_task_prompt

            inputs = processor(text=model_prompt, images=image, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}

            logger.info(f"Florence-2 model inputs prepared. Input IDs shape: {inputs['input_ids'].shape}, Pixel values shape: {inputs['pixel_values'].shape}")
            if device.type == 'mps':
                logger.info(f"MPS memory before generation: Allocated={torch.mps.current_allocated_memory() / (1024**2):.2f}MB, Cached={torch.mps.current_cached_memory() / (1024**2):.2f}MB")
            
            generated_ids = model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=256,
                num_beams=4
            )
            logger.info("Florence-2 model generation complete.")
            if device.type == 'mps':
                logger.info(f"MPS memory after generation: Allocated={torch.mps.current_allocated_memory() / (1024**2):.2f}MB, Cached={torch.mps.current_cached_memory() / (1024**2):.2f}MB")

            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
            parsed_answer = processor.post_process_generation(
                generated_text,
                task=base_task_prompt,
                image_size=(image.width, image.height)
            )
            caption = parsed_answer.get(base_task_prompt, "Could not generate caption.")

        elif "qwen" in HF_MODEL_NAME.lower():
            # Qwen-VL specific logic
            text_prompt = "Describe the image."
            if user_prompt:
                text_prompt = f"Please describe this image, paying special attention to: {user_prompt}"

            messages = [
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {
                            "type": "text",
                            "text": text_prompt
                        }
                    ]
                }
            ]

            prompt = processor.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = processor(text=[prompt], images=[image], return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}

            logger.info(f"""Qwen model inputs prepared. Input IDs shape: {inputs['input_ids'].shape}, Pixel values shape: {inputs['pixel_values'].shape if 'pixel_values' in inputs else 'N/A'}""")
            if device.type == 'mps':
                logger.info(f"MPS memory before generation: Allocated={torch.mps.current_allocated_memory() / (1024**2):.2f}MB, Cached={torch.mps.current_cached_memory() / (1024**2):.2f}MB")

            generated_ids = model.generate(**inputs, max_new_tokens=256)
            logger.info("Qwen model generation complete.")
            if device.type == 'mps':
                logger.info(f"MPS memory after generation: Allocated={torch.mps.current_allocated_memory() / (1024**2):.2f}MB, Cached={torch.mps.current_cached_memory() / (1024**2):.2f}MB")
            
            input_token_len = inputs["input_ids"].shape[1]
            generated_text = processor.batch_decode(generated_ids[:, input_token_len:], skip_special_tokens=True)[0]
            caption = generated_text.strip()

        else:
            return f"Unsupported model type: {HF_MODEL_NAME}"

        logger.info(f"Generated caption: '{caption}'")
        return caption
    finally:
        unload_model_and_processor()


import base64

def image_to_base64(image: Image.Image) -> str:
    """Converts a PIL image to a base64 encoded string."""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

async def handle_image(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handles incoming photos and generates a caption."""
    user_id = update.effective_user.id
    if ALLOWED_USER_IDS and user_id not in ALLOWED_USER_IDS:
        logger.warning(f"Unauthorized access attempt by user_id: {user_id}")
        await context.bot.send_message(
            chat_id=update.effective_chat.id,
            text="Sorry, you are not authorized to use this bot."
        )
        return

    chat_id = update.effective_chat.id
    logger.info(f"Received image from chat_id: {chat_id}")

    user_prompt = update.message.caption

    await context.bot.send_message(chat_id=chat_id, text="Processing your image...")

    # Get the largest photo sent
    photo_file = await update.message.photo[-1].get_file()

    # Download the photo into a BytesIO object
    file_bytes = await photo_file.download_as_bytearray()
    image_stream = BytesIO(file_bytes)

    try:
        image = Image.open(image_stream).convert("RGB")
        caption = generate_caption(image, user_prompt)

        user_data = load_user_data()
        user_suffix = user_data.get('users', {}).get(user_id, {}).get('suffix', DEFAULT_POLITE_NOTICE)

        await context.bot.send_message(chat_id=chat_id, text=f"{caption}\n\n{user_suffix}")
    except Exception as e:
        logger.error(f"An error occurred while processing the image: {e}")
        await context.bot.send_message(chat_id=chat_id, text="Sorry, I couldn't process that image.")


def main():
    """Start the bot."""
    if not TELEGRAM_BOT_TOKEN:
        logger.error("TELEGRAM_BOT_TOKEN not found. Please create a .env file and add it.")
        return

    if "qwen" in HF_MODEL_NAME.lower() and not SYSTEM_PROMPT:
        logger.error("SYSTEM_PROMPT not found in .env file. It is required for Qwen models.")
        return

    application = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()

    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("setsuffix", set_suffix))
    application.add_handler(MessageHandler(filters.PHOTO, handle_image))

    logger.info("Bot is running. Press Ctrl-C to stop.")
    application.run_polling()


if __name__ == '__main__':
    main()
